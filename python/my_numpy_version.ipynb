{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "\n",
    "mnist_data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=data_dir, train=True, transform=mnist_data_transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=data_dir, train=False, transform=mnist_data_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(\n",
    "        self, input_features: int, output_features: int, with_bias: bool = True\n",
    "    ):\n",
    "        self.weights = Linear.initialize_weights(\n",
    "            input_size=input_features, output_size=output_features\n",
    "        )\n",
    "        self.grad_weights = np.empty_like(self.weights)\n",
    "        self.grad_input = np.empty((batch_size,input_features))\n",
    "        if with_bias:\n",
    "            self.bias = Linear.initialize_bias(output_size=output_features)\n",
    "            self.grad_bias = np.empty_like(self.bias)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad_weights.fill(0)\n",
    "        self.grad_bias.fill(0)\n",
    "        self.grad_input.fill(0)\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        assert x.shape[1] == self.weights.shape[0]\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "\n",
    "    def initialize_weights(input_size, output_size):\n",
    "        return np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "\n",
    "    def initialize_bias(output_size):\n",
    "        return np.empty((1, output_size))\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(\n",
    "        self,\n",
    "        grad_output: np.ndarray,\n",
    "        x: np.ndarray,\n",
    "    ):\n",
    "        # 计算的是这个线性层有变化，然后对线性层相关变量的梯度\n",
    "        grad_weights = x.T @ grad_output\n",
    "        # TODO: 搞清楚grad_output的维度\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_input = grad_output @ self.weights.T\n",
    "        self.grad_weights[:] = grad_weights[:]\n",
    "        self.grad_bias[:] = grad_bias[:]\n",
    "        self.grad_input[:] = grad_input[:]\n",
    "        return grad_input, grad_weights, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray, input: np.ndarray):\n",
    "        return grad_output * (input > 0).astype(float)\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input: np.ndarray):\n",
    "        return self.forward(input=input)\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        exp_input = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        return exp_input / np.sum(exp_input, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, grad_output: np.ndarray, input: np.ndarray):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.softmax: Softmax = Softmax()\n",
    "        pass\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray):\n",
    "        \"\"\"\n",
    "        类似pytorch中的版本\n",
    "        input: (N, C)，在这个jupyter notebook中，C=10，N应该会等于4,这里是没有归一化的神经网络上一层输出\n",
    "        target: (N, ),也就是预测的标签,预测的标签这个输入形式意味着其等价的（N,C)这种维度的向量表示中，每一个样本只能对应一个种类，也就是只有一个1,其余的全是0\n",
    "        返回一个非负数，当且仅当input中预测完全正确的时候才等于0\n",
    "        \"\"\"\n",
    "        # 对input归一化\n",
    "        input_probs = self.softmax(input=input)\n",
    "        # 找到每一个样本中正确的那一个分类的概率（因为错误的分类的真值是0,所以最终不会被累加）\n",
    "        assert len(input_probs) == len(target)\n",
    "        correct_class_probs = input_probs[np.arange(len(input_probs)), target]\n",
    "        # 这意味着如下代码：\n",
    "        # correct_class_probs = np.zeros_like(target)\n",
    "        # for i, input_prob, target_label in enumerate(zip(input_probs, target)):\n",
    "        #     #将正确结果那一个的预测分类概率记下来\n",
    "        #     correct_class_probs[i] = input_prob[target_label]\n",
    "\n",
    "        # 返回交叉熵损失\n",
    "        return -np.mean(np.log(correct_class_probs))\n",
    "    def __call__(self, input: np.ndarray, target: np.ndarray):\n",
    "        return self.forward(input=input,target=target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Optimizer:\n",
    "    learning_rate: float\n",
    "    linear_weight1: np.ndarray\n",
    "    linear_bias1: np.ndarray\n",
    "    linear_weight2: np.ndarray\n",
    "    linear_bias2: np.ndarray\n",
    "    linear_grid_weight1: np.ndarray\n",
    "    linear_grid_bias1: np.ndarray\n",
    "    linear_grid_weight2: np.ndarray\n",
    "    linear_grid_bias2: np.ndarray\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.linear_weight1[:] -= self.learning_rate * self.linear_grid_weight1\n",
    "        self.linear_bias1[:] -= self.learning_rate * self.linear_grid_bias1\n",
    "        self.linear_grid_weight2[:] -= self.learning_rate * self.linear_grid_weight2\n",
    "        self.linear_grid_bias2[:] -= self.learning_rate * self.linear_grid_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_features: int, hidden_features: int, num_classes: int):\n",
    "        self.fc1: Linear = Linear(\n",
    "            input_features=input_features, output_features=hidden_features\n",
    "        )\n",
    "        self.relu: ReLU = ReLU()\n",
    "        self.fc2: Linear = Linear(\n",
    "            input_features=hidden_features, output_features=num_classes\n",
    "        )\n",
    "        self.softmax: Softmax = Softmax()\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        return self.forward(x=x)\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        fc1_input = x.reshape(batch_size, 28 * 28)\n",
    "        fc1_output = self.fc1(fc1_input)\n",
    "        relu_output = self.relu(fc1_output)\n",
    "        fc2_output = self.fc2(relu_output)\n",
    "        return fc2_output, (fc1_input, fc1_output, relu_output)\n",
    "\n",
    "    def backward(self, grad_output, cache):\n",
    "        fc1_input, fc1_output, relu_output = cache\n",
    "        # 计算线性层fc2的梯度\n",
    "        grad_fc2, grad_weights2, grad_bias2 = self.fc2.backward(\n",
    "            grad_output=grad_output,\n",
    "            x=relu_output,  # 这里与forward的时候的输入relu_output要对应\n",
    "        )\n",
    "        # 计算ReLU的梯度\n",
    "        grad_relu = self.relu.backward(\n",
    "            grad_output=grad_fc2,\n",
    "            input=fc1_output,\n",
    "        )\n",
    "        # 计算线性层fc1的梯度\n",
    "        grad_fc1, grad_weights1, grad_bias1 = self.fc1.backward(\n",
    "            grad_output=grad_relu,\n",
    "            x=fc1_input,\n",
    "        )\n",
    "        return grad_weights1, grad_bias1, grad_weights2, grad_bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MLP, train_loader: DataLoader, learning_rate: float, epochs: int):\n",
    "    optimizer = Optimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        linear_weight1=model.fc1.weights,\n",
    "        linear_bias1=model.fc1.bias,\n",
    "        linear_weight2=model.fc2.weights,\n",
    "        linear_bias2=model.fc2.bias,\n",
    "        linear_grid_weight1=model.fc1.grad_weights,\n",
    "        linear_grid_bias1=model.fc1.grad_bias,\n",
    "        linear_grid_weight2=model.fc2.grad_weights,\n",
    "        linear_grid_bias2=model.fc2.grad_bias,\n",
    "    )\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            y_pred, cache = model(x=images)\n",
    "            #计算loss\n",
    "            ce_loss=criterion(input=y_pred,target=labels)\n",
    "            print(f\"loss = {ce_loss}\")\n",
    "            softmax_probs = model.softmax(input=y_pred)\n",
    "            y_true_one_hot = np.zeros_like(y_pred)\n",
    "            y_true_one_hot[range(batch_size), labels] = 1\n",
    "\n",
    "            grad_output = softmax_probs - y_true_one_hot\n",
    "            model.backward(grad_output=grad_output, cache=cache)\n",
    "            optimizer.update_weights()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "model = MLP(\n",
    "    input_features=input_size, hidden_features=hidden_size, num_classes=output_size\n",
    ")\n",
    "epochs=3\n",
    "learning_rate=1e-3\n",
    "train(model=model,train_loader=train_loader,learning_rate=learning_rate,epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
