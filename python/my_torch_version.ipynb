{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b119e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef8599f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "TRAIN_SIZE=1e4\n",
    "epochs=3\n",
    "iters_per_epoch=int(TRAIN_SIZE//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b52b5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备minist数据集\n",
    "data_dir = \"data\"\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # Mean and std of MNIST\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ef99d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=data_dir, train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=data_dir, train=False, transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3c5e2251",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cf74287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.reshape(batch_size, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a8d8df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLP(in_features=784,hidden_features=256,num_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d606f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.functional,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "):\n",
    "    model.train()\n",
    "    running_loss: float = 0\n",
    "    # for i in range(iters_per_epoch):\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data.cuda())\n",
    "        loss = criterion(outputs, target.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99 or i == 0:\n",
    "            print(f\"Iteration {i}, Loss {running_loss/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4d410a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ceebbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss 2.370542049407959\n",
      "Iteration 99, Loss 2.238724137544632\n",
      "Iteration 199, Loss 2.160097403526306\n",
      "Iteration 299, Loss 2.0759207757314044\n",
      "Iteration 399, Loss 1.9999788796901703\n",
      "Iteration 499, Loss 1.9289768240451812\n",
      "Iteration 599, Loss 1.8484020765622458\n",
      "Iteration 699, Loss 1.7785841651473726\n",
      "Iteration 799, Loss 1.7061303236335517\n",
      "Iteration 899, Loss 1.6388714031378429\n",
      "Iteration 999, Loss 1.5775710051059724\n",
      "Iteration 1099, Loss 1.5260332587361336\n",
      "Iteration 1199, Loss 1.4731472800920407\n",
      "Iteration 1299, Loss 1.4239216938385597\n",
      "Iteration 1399, Loss 1.3844142674122537\n",
      "Iteration 1499, Loss 1.3450427582859994\n",
      "Iteration 1599, Loss 1.3101327836792915\n",
      "Iteration 1699, Loss 1.2784056954261134\n",
      "Iteration 1799, Loss 1.246722096958094\n",
      "Iteration 1899, Loss 1.2145366725953002\n",
      "Iteration 1999, Loss 1.1866362716853618\n",
      "Iteration 2099, Loss 1.1573888855037235\n",
      "Iteration 2199, Loss 1.129998024356636\n",
      "Iteration 2299, Loss 1.1080167867534834\n",
      "Iteration 2399, Loss 1.0873312521260232\n",
      "Iteration 2499, Loss 1.068446127229929\n",
      "Iteration 2599, Loss 1.0510640116523091\n",
      "Iteration 2699, Loss 1.0329538320860376\n",
      "Iteration 2799, Loss 1.01434683607359\n",
      "Iteration 2899, Loss 0.9946848205528382\n",
      "Iteration 2999, Loss 0.9817362617589533\n",
      "Iteration 3099, Loss 0.968464222465071\n",
      "Iteration 3199, Loss 0.952135886043543\n",
      "Iteration 3299, Loss 0.9376150220079404\n",
      "Iteration 3399, Loss 0.9256152925859479\n",
      "Iteration 3499, Loss 0.9132693629222257\n",
      "Iteration 3599, Loss 0.9008166878173749\n",
      "Iteration 3699, Loss 0.8884560416840218\n",
      "Iteration 3799, Loss 0.8781978046041178\n",
      "Iteration 3899, Loss 0.867493905298985\n",
      "Iteration 3999, Loss 0.8572528833393007\n",
      "Iteration 4099, Loss 0.8481093896816416\n",
      "Iteration 4199, Loss 0.8386804632594188\n",
      "Iteration 4299, Loss 0.8300872649564299\n",
      "Iteration 4399, Loss 0.8211836659654298\n",
      "Iteration 4499, Loss 0.8129752024710178\n",
      "Iteration 4599, Loss 0.8036619354177105\n",
      "Iteration 4699, Loss 0.7952642148685583\n",
      "Iteration 4799, Loss 0.7857498363809039\n",
      "Iteration 4899, Loss 0.7775845016060131\n",
      "Iteration 4999, Loss 0.771224485912919\n",
      "Iteration 5099, Loss 0.7633636584622311\n",
      "Iteration 5199, Loss 0.7564985915943264\n",
      "Iteration 5299, Loss 0.7493947703222621\n",
      "Iteration 5399, Loss 0.74373097725592\n",
      "Iteration 5499, Loss 0.7372768262038853\n",
      "Iteration 5599, Loss 0.7306147873045744\n",
      "Iteration 5699, Loss 0.7241777409596794\n",
      "Iteration 5799, Loss 0.7180947842696233\n",
      "Iteration 5899, Loss 0.7123069726268493\n",
      "Iteration 5999, Loss 0.7071046290191201\n",
      "Iteration 6099, Loss 0.7027341212214688\n",
      "Iteration 6199, Loss 0.6982763767240929\n",
      "Iteration 6299, Loss 0.6929628563120902\n",
      "Iteration 6399, Loss 0.6878129289425852\n",
      "Iteration 6499, Loss 0.682685041304821\n",
      "Iteration 6599, Loss 0.6776404237858669\n",
      "Iteration 6699, Loss 0.6722915682039543\n",
      "Iteration 6799, Loss 0.6673829663606525\n",
      "Iteration 6899, Loss 0.662445291088948\n",
      "Iteration 6999, Loss 0.6582081818337153\n",
      "Iteration 7099, Loss 0.653822191981074\n",
      "Iteration 7199, Loss 0.6496318525007356\n",
      "Iteration 7299, Loss 0.6454960046245474\n",
      "Iteration 7399, Loss 0.6412859811708079\n",
      "Iteration 7499, Loss 0.6380778076195468\n",
      "Iteration 7599, Loss 0.634293941540619\n",
      "Iteration 7699, Loss 0.6305073867306452\n",
      "Iteration 7799, Loss 0.6266658258446468\n",
      "Iteration 7899, Loss 0.6223267397936434\n",
      "Iteration 7999, Loss 0.6196247918490553\n",
      "Iteration 8099, Loss 0.6155790432708131\n",
      "Iteration 8199, Loss 0.6124658279259485\n",
      "Iteration 8299, Loss 0.6084141391978027\n",
      "Iteration 8399, Loss 0.6049373155199214\n",
      "Iteration 8499, Loss 0.602236566342523\n",
      "Iteration 8599, Loss 0.599538515780316\n",
      "Iteration 8699, Loss 0.5974050830992945\n",
      "Iteration 8799, Loss 0.5951278188847937\n",
      "Iteration 8899, Loss 0.5919334209122266\n",
      "Iteration 8999, Loss 0.5893548294688679\n",
      "Iteration 9099, Loss 0.5863872200988008\n",
      "Iteration 9199, Loss 0.5830052544851281\n",
      "Iteration 9299, Loss 0.5802147822250783\n",
      "Iteration 9399, Loss 0.5778057346564837\n",
      "Iteration 9499, Loss 0.5749974068273349\n",
      "Iteration 9599, Loss 0.5731671955535906\n",
      "Iteration 9699, Loss 0.5710481739830053\n",
      "Iteration 9799, Loss 0.5690831684312136\n",
      "Iteration 9899, Loss 0.5672334666240189\n",
      "Iteration 9999, Loss 0.5650436690718401\n",
      "Iteration 10099, Loss 0.5626328407831509\n",
      "Iteration 10199, Loss 0.5601636995543616\n",
      "Iteration 10299, Loss 0.5574385308731546\n",
      "Iteration 10399, Loss 0.5553384078767312\n",
      "Iteration 10499, Loss 0.5530248678657005\n",
      "Iteration 10599, Loss 0.5503968116093234\n",
      "Iteration 10699, Loss 0.5485662296727283\n",
      "Iteration 10799, Loss 0.5463169390666385\n",
      "Iteration 10899, Loss 0.5446297549714821\n",
      "Iteration 10999, Loss 0.5424883261845785\n",
      "Iteration 11099, Loss 0.5404618034776518\n",
      "Iteration 11199, Loss 0.5381743378965101\n",
      "Iteration 11299, Loss 0.5358950503414035\n",
      "Iteration 11399, Loss 0.5340227736744687\n",
      "Iteration 11499, Loss 0.5317894032182496\n",
      "Iteration 11599, Loss 0.529723105271676\n",
      "Iteration 11699, Loss 0.5279430522333879\n",
      "Iteration 11799, Loss 0.5258935592285263\n",
      "Iteration 11899, Loss 0.5236668408440207\n",
      "Iteration 11999, Loss 0.5224468995757245\n",
      "Iteration 12099, Loss 0.5204229844010566\n",
      "Iteration 12199, Loss 0.5190134065112167\n",
      "Iteration 12299, Loss 0.5173860138607574\n",
      "Iteration 12399, Loss 0.5153757377592059\n",
      "Iteration 12499, Loss 0.5139494627934508\n",
      "Iteration 12599, Loss 0.5124923258822315\n",
      "Iteration 12699, Loss 0.5109727301496066\n",
      "Iteration 12799, Loss 0.5090231100761412\n",
      "Iteration 12899, Loss 0.5080238217406892\n",
      "Iteration 12999, Loss 0.5060725208211224\n",
      "Iteration 13099, Loss 0.5045333393141165\n",
      "Iteration 13199, Loss 0.5030874751000771\n",
      "Iteration 13299, Loss 0.5015625693646253\n",
      "Iteration 13399, Loss 0.5000891214808268\n",
      "Iteration 13499, Loss 0.49843539017297467\n",
      "Iteration 13599, Loss 0.49669524334816206\n",
      "Iteration 13699, Loss 0.4949195209250235\n",
      "Iteration 13799, Loss 0.49326460808486866\n",
      "Iteration 13899, Loss 0.4921340763702714\n",
      "Iteration 13999, Loss 0.4908886184762731\n",
      "Iteration 14099, Loss 0.48908952427795155\n",
      "Iteration 14199, Loss 0.48763032569154824\n",
      "Iteration 14299, Loss 0.4862412426750488\n",
      "Iteration 14399, Loss 0.48501574221253074\n",
      "Iteration 14499, Loss 0.4840076474687887\n",
      "Iteration 14599, Loss 0.4831683788280328\n",
      "Iteration 14699, Loss 0.4818667564276435\n",
      "Iteration 14799, Loss 0.4810291271231364\n",
      "Iteration 14899, Loss 0.479770169441882\n",
      "Iteration 14999, Loss 0.47845942684557907\n"
     ]
    }
   ],
   "source": [
    "train(model,criterion,optimizer,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac62258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model:nn.Module,test_data_loader:DataLoader):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for data in test_data_loader:\n",
    "            images,labels=data\n",
    "            outputs=model(images.cuda())\n",
    "            _,predicted=torch.max(outputs.data,1)\n",
    "            total+=labels.size(0)\n",
    "            correct+=(predicted==labels.cuda()).sum().item()\n",
    "    print(f\"Accuracy: {100*correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "558266b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.74\n"
     ]
    }
   ],
   "source": [
    "evaluate(model=model,test_data_loader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
